// This file was generated by BAML: do not edit it.
// Instead, edit the BAML source files.
//
// Learn more at https://docs.boundaryml.com

//! Embedded BAML source files.

use std::collections::HashMap;
use std::sync::OnceLock;

static FILE_MAP: OnceLock<HashMap<String, String>> = OnceLock::new();

/// Get the embedded BAML source files.
pub fn get_baml_files() -> &'static HashMap<String, String> {
    FILE_MAP.get_or_init(|| {
        let mut m = HashMap::new();

        m.insert("agent.baml".to_string(), "// ChoirOS Chat Agent Functions\n// BAML-powered agent functions for planning and response synthesis\n\n// Main agent planning function - determines what to do based on conversation\nfunction PlanAction(\n  messages: Message[],\n  system_context: string,\n  available_tools: string\n) -> AgentPlan {\n  client ClaudeBedrock\n  prompt #\"\n    You are ChoirOS, an intelligent AI assistant in a web desktop environment.\n\n    {{ system_context }}\n\n    Available Tools:\n    {{ available_tools }}\n\n    Conversation History:\n    {{ messages }}\n\n    Think step by step about what to do:\n    1. Analyze the user's request\n    2. Determine if you need to use tools or can respond directly\n    3. If using tools, specify which ones with clear reasoning\n    4. Provide a confidence score (0.0-1.0) in your plan\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Synthesize final response after tool execution\nfunction SynthesizeResponse(\n  user_prompt: string,\n  tool_results: ToolResult[],\n  conversation_context: string\n) -> string {\n  client ClaudeBedrock\n  prompt #\"\n    You are ChoirOS, an AI assistant. Synthesize a helpful, natural response.\n\n    User Request: {{ user_prompt }}\n\n    Conversation Context:\n    {{ conversation_context }}\n\n    Tool Results:\n    {{ tool_results }}\n\n    Based on the user's request and the tool results, provide a clear, helpful response.\n    Acknowledge what was done and answer the user's question concisely.\n  \"#\n}\n\n// Quick response for simple queries (no tool use)\nfunction QuickResponse(\n  user_message: string,\n  conversation_history: string\n) -> string {\n  client GLM47\n  prompt #\"\n    You are ChoirOS, a helpful AI assistant.\n\n    Conversation History:\n    {{ conversation_history }}\n\n    User: {{ user_message }}\n\n    Provide a brief, helpful response.\n  \"#\n}\n".to_string());

        m.insert("clients.baml".to_string(), "// ChoirOS Chat Agent Clients\n// Define both model providers: ClaudeBedrock (AWS) and GLM47 (Z.ai)\n\n// AWS Bedrock Claude configuration\nclient<llm> ClaudeBedrock {\n  provider aws-bedrock\n  retry_policy Exponential\n  options {\n    model \"us.anthropic.claude-opus-4-5-20251101-v1:0\"\n    region \"us-east-1\"\n    // AWS credentials auto-detected from environment:\n    // AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION\n  }\n}\n\n// Z.ai GLM4.7 configuration\nclient<llm> GLM47 {\n  provider openai-generic\n  retry_policy Exponential\n  options {\n    model \"glm-4.7\"\n    base_url \"https://api.z.ai/api/anthropic\"\n    api_key env.ZAI_API_KEY\n  }\n}\n\n// Retry policies\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n".to_string());

        m.insert("generators.baml".to_string(), "// BAML Generator for Rust - ChoirOS Chat Agent\n// This generates Rust code for BAML functions\n\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"rust\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../sandbox/src/baml_client\"\n\n    // The version of the BAML package you have installed\n    version \"0.217.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n".to_string());

        m.insert("resume.baml".to_string(), "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like \"client CustomGPT5\" or \"client CustomSonnet4\"\n  client \"openai-responses/gpt-5-mini\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n".to_string());

        m.insert("types.baml".to_string(), "// ChoirOS Chat Agent Types\n// Core types for BAML-powered chat agent with tool execution\n\n// Message in conversation\nclass Message {\n  role string\n  content string\n}\n\n// Individual tool call from the agent\nclass AgentToolCall {\n  tool_name string\n  tool_args string\n  reasoning string\n}\n\n// The agent's plan - thinking, tool calls, and final response\nclass AgentPlan {\n  thinking string\n  tool_calls AgentToolCall[]\n  final_response string?\n  confidence float\n}\n\n// Result of executing a tool\nclass ToolResult {\n  tool_name string\n  success bool\n  output string\n  error string?\n}\n\n// Streaming chunk types for WebSocket\nclass StreamChunk {\n  chunk_type string // \"thinking\", \"tool_call\", \"tool_result\", \"response\", \"error\"\n  content string\n}\n".to_string());

        m
    })
}
